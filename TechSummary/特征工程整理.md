# 特征工程整理

## 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。
## 特征工程：目的是最大限度地从原始数据中提取提取特征一共模型和算法使用的一项工程活动。
## 数值变量标准化
* 不同的数据的Scale不一样，所以需要标准化。比如身高和体重。无量纲化可以帮助解决这个问题。
* 不做标准化，有些算法会死得很惨，比如SVM，神经网络、K-means之类。标准化的一种方法是均值方差法。
* 不是什么时候都需要标准化，比如物理意义非常明确的经纬度，如果标准化，其本身的意义就丢失。
* 标准化≠归一化，有些算法不需要标准化，比如决策树
## 连续特征的离散化
* 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0,1特征交给逻辑回归模型，这样做的优势：
1. 离散特征的增加和减少都很容易，易于模型的快速迭代。
2. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。
3. 离散化的特征对异常数据有很强的鲁棒性。
4. 逻辑回归属于广义的线性模型，表达能力有限；单变量离散化为N个后，每个变量都有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。
5. 离散化后可以进行特征交叉组合，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。
6. 特征离散化后，模型会更加的稳定，但是划分区间是一门学问。
7. 特征离散化后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

## 定性特征不能直接使用
某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，可以将一个特征扩展为扩展为N种特征。
其实该部分是离散特征连续化，最简单的方式就是one-hot。
one-hot会破坏语义，可以引入大量的Embedding技术。

## 存在缺失值
存在缺失值得情况下需要进行处理，一般根据缺失比例和缺失含义进行填充。
* 缺失比例大：直接不使用
* 缺失比例小：有含义得话可以填固定值(0,-1),没有含义可以通过统计量进行填充
* 可以采用模型预估缺失值进行填充

## 特征选择
当数据预处理完之后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面来选择特征：
* 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本没有差异，这个特征对于样本的区分并没什么用。
* 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优先选择。

## 特征选择的方法：
Filter：过滤法：按照发散性或者相关性对特征评分，设定阈值或者选择阈值的个数，选择特征。
Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
Embedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从小到大选择特征。类似于Filter方法，但是通过训练来确定特征的优劣。

## 样本不平衡情况
特征高频和低频都是需要特别处理的地方，可以过抽样，欠抽型或者人工补的方法（常用SMOTE）（最近解决的时候人工补效果很差）。具体问题具体的分析，跟样本的分布有关，概率论需要好好学。

## 降维很重要
好的降维能提高学习器性能同时避免过拟合。
方法：
* 人肉：SIFT，VLAD，HOG，GIST，LBP。 通过分析相关性自己选择
* 模型：Sparse Coding，Auto Encoders，Restricted Boltzmann Machines，PCA，ICA，K-means等

## 直觉和额外的特征
1. 针对原始数据，可以利用自己的特长手动或自动生成直觉和额外的特征。比如文本问题，可以写个自动算法生成单词长度，元音个数等。
2. 数据分析师可能发生噪音中信号。
3. 最后这一点也是我们常说的开脑洞，需要我们培养数据敏感能力。路漫漫兮，加油吧
4. 了解业务最重要！了解业务最重要！了解业务最重要！

## 模型和特征得权衡
模型是使用离散特征还是连续特征，其实 是一个“海量离散特征+简单模型”同“少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以使用连续特征加深度学习。就看喜欢折腾特征还是折腾模型了。通常来说前者容易，有成功经验；后者目前看很赞，能走多远拭目以待。